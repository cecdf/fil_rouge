{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding frequent itemsets with the Apriori algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loadDataSet()** creates a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet = loadDataSet()\n",
    "dataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 is a candidate itemset of\n",
    "size one. In the Apriori algorithm, we create C1, and then we’ll scan the dataset to see\n",
    "if these one itemsets meet our minimum support requirements. The itemsets that do meet our minimum requirements become L1. L1 then gets combined to become C2\n",
    "and C2 will get filtered to become L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatC1(dataSet): \n",
    "    C1 = []\n",
    "    for transaction in dataSet:\n",
    "        for item in transaction: \n",
    "            if not[item] in C1: \n",
    "                C1.append([item])\n",
    "    C1.sort()\n",
    "    return map(frozenset, C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**scanD()** takes three arguments:\n",
    "a dataset, Ck, a list of candidate sets, and minSupport, which is the minimum\n",
    "support you’re interested in. This is the function you’ll use to generate L1 from C1.\n",
    "Additionally, this function returns a dictionary with support values for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanD(D, Ck, minSupport): \n",
    "    \n",
    "    ssCnt = {}\n",
    "    \n",
    "    Dlist = list(D)\n",
    "    Cklist = list(Ck)\n",
    "    \n",
    "    for tid in Dlist:\n",
    "        for can in Cklist: \n",
    "            if can.issubset(tid): \n",
    "                if can not in ssCnt: \n",
    "                    ssCnt[can] = 1\n",
    "                else: \n",
    "                    ssCnt[can] += 1\n",
    "    numItems = float(len(Dlist))\n",
    "    \n",
    "    retList = []\n",
    "    supportData = {}\n",
    "    \n",
    "    for key in ssCnt: \n",
    "        support = ssCnt[key]/numItems \n",
    "        if support >= minSupport: \n",
    "            retList.insert(0, key) \n",
    "        supportData[key] = support\n",
    "    \n",
    "    return retList, supportData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = map(set, dataSet)\n",
    "C1 = creatC1(dataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four items make up our L1 list, that is, the list of one-item sets that occur in at\n",
    "least 50% of all transactions. Item 4 didn’t make the minimum support level, so it’s\n",
    "not a part of L1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1, suppData0 = scanD(D, C1, 0.5)\n",
    "L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **aprioriGen()** will take a list of frequent itemsets, Lk, and the size of\n",
    "the itemsets, k, to produce Ck.\n",
    "\n",
    "\n",
    "Everything gets wrapped up in the **apriori() function**. You give this a dataset and\n",
    "a support number, and it will generate a list of candidate itemsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aprioriGen(Lk, k): \n",
    "    retList = []\n",
    "    lenLk = len(Lk)\n",
    "    \n",
    "    for i in range(lenLk): \n",
    "        for j in range(i+1, lenLk): \n",
    "            L1 = list(Lk[i])[:k-2].sort()\n",
    "            L2 = list(Lk[j])[:k-2].sort()\n",
    "            if L1 == L2: \n",
    "                retList.append(Lk[i] | Lk[j])\n",
    "    return map(frozenset, retList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(dataSet, minSupport = 0.5): \n",
    "    C1 = creatC1(dataSet)\n",
    "    D = map(set, dataSet)\n",
    "    L1, supportData = scanD(D, C1, minSupport)\n",
    "    L = [L1]\n",
    "    k = 2\n",
    "    \n",
    "    while(len(L[k-2]) > 0):\n",
    "        D = map(set, dataSet)\n",
    "        Ck = aprioriGen(L[k-2], k)\n",
    "        Lk, supK = scanD(D, Ck, minSupport)\n",
    "        supportData.update(supK)\n",
    "        L.append(Lk)\n",
    "        k += 1\n",
    "    \n",
    "    return L, supportData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L contains some lists of frequent itemsets that met a minimum support of 0.5(minsupport = 0.5 by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})],\n",
       " [frozenset({2, 3}), frozenset({3, 5}), frozenset({2, 5}), frozenset({1, 3})],\n",
       " [frozenset({2, 3, 5})],\n",
       " []]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, suppData = apriori(dataSet)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try algorithm by support = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[frozenset({5}), frozenset({2}), frozenset({3})], [frozenset({2, 5})], []]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, suppData = apriori(dataSet, minSupport = 0.7)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining association rules from frequent item sets\n",
    "\n",
    "The **generateRules()** function takes three inputs: a list of frequent itemsets, a dictionary\n",
    "of support data for those itemsets, and a minimum confidence threshold. It’s\n",
    "going to generate a list of rules with confidence values that we can sort through later.\n",
    "These rules are stored in bigRuleList."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRules(L, supportData, minConf=0.7): \n",
    "    \n",
    "    bigRuleList = []\n",
    "    \n",
    "    for i in range(1, len(L)):\n",
    "        for freqSet in L[i]:\n",
    "            H1 = [frozenset([item]) for item in freqSet]\n",
    "            if (i > 1):\n",
    "                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "            else:\n",
    "                calcConf(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "    return bigRuleList   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re interested in calculating the confidence of a rule and then finding out\n",
    "which rules meet the minimum confidence. All of this is done in **calcConf()**,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcConf(freqSet, H, supportData, brl, minConf=0.7):\n",
    "    \n",
    "    prunedH = []\n",
    "    \n",
    "    for conseq in H:\n",
    "        conf = supportData[freqSet]/supportData[freqSet-conseq] #calc confidence\n",
    "        if conf >= minConf:\n",
    "            print(freqSet-conseq,'-->',conseq,'conf:',conf)\n",
    "            brl.append((freqSet-conseq, conseq, conf))\n",
    "            prunedH.append(conseq)\n",
    "    return prunedH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate more association rules from our initial itemset, you use the\n",
    "**rulesFromConseq() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\n",
    "    \n",
    "    m = len(H[0])\n",
    "    #H中的频繁集大小\n",
    "    \n",
    "    '''\n",
    "    接下来查看该频繁项集是否大到可以移除大小为m的子集。如果可以的话，则将其移除。\n",
    "    '''\n",
    "    \n",
    "    if (len(freqSet) > (m + 1)): \n",
    "        \n",
    "        Hmp1 = aprioriGen(H, m+1)\n",
    "        #可以使用aprioriGen()来生成H中元素的无重复组合。\n",
    "        \n",
    "        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)\n",
    "        #可以利用calcConf()来测试它们的可信度以确定规则是否满足要求\n",
    "        \n",
    "        if (len(Hmp1) > 1): \n",
    "        #如果不止一条规则满足要求，递归调用来判断是否可以进一步组合这些规则。\n",
    "            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})],\n",
       " [frozenset({2, 3}), frozenset({3, 5}), frozenset({2, 5}), frozenset({1, 3})],\n",
       " [frozenset({2, 3, 5})],\n",
       " []]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, suppData = apriori(dataSet, minSupport = 0.5)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({5}) --> frozenset({2}) conf: 1.0\n",
      "frozenset({2}) --> frozenset({5}) conf: 1.0\n",
      "frozenset({1}) --> frozenset({3}) conf: 1.0\n",
      "frozenset({5}) --> frozenset({2, 3}) conf: 2.0\n",
      "frozenset({3}) --> frozenset({2, 5}) conf: 2.0\n",
      "frozenset({2}) --> frozenset({3, 5}) conf: 2.0\n"
     ]
    }
   ],
   "source": [
    "rules = generateRules(L, suppData, minConf = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
